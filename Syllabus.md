## Syllabus

# Module 1: Introduction of MLOps

☛ What is MLOps?

☛ Importance of MLOps in the ML lifecycle

☛ Key principles and practices of MLOps Lifecycle: From ☛ Model Development to Deployment

☛ Comparison of MLOps with DevOps

☛ MLOps Workflow and Components:

---↪️ (CI-CD)

---↪️ (CM/CT)

---↪️ (Experiment Tracking)

---↪️ (Model Registries)

---↪️ (Feature Stores)

---↪️ (Different Model Deployment Strategies)

---↪️ (Data Drift)

---↪️ (Model Drift)

☛ MLOps in the Context of Large Scale Machine Learning Workflows

# Module 2: Linux & Git Fundamentals

☛ Introduction to Linux and Git:

☛ Setting Up WSL2/Linux VM/Linux OS

☛ Basic Linux commands: Navigation || File Management || Process Management

☛ Installing and Managing Software in Linux

☛ System Monitoring and Performance Tuning in Linux

☛ Shell Scripting Basics

☛ Git basics: Version Control || Branchin || Merging

☛ Git workflow: Commit || Push || Pull || Forks || Pull Requests || Code reviews

☛ GitHub Actions

# Module 3: Building Machine Learning Workflow and Project Setup

☛ **GitHub and its role in machine learning projects:**

---↪️ Setting up a project template with GitHub: Best practices for organizing and structuring code repositories

☛ **Modular Workflow Introduction and Implementation:**

---↪️ Understanding the importance of modular workflows in machine learning

---↪️ Implementing modular workflows using Python and other tools

---↪️ Best practices for modular workflow design and implementation

☛ **Understanding the Training Pipeline and Its Components:**

☛ **Creating Prediction Pipeline and End Point Creation**

---↪️ Understanding the importance of prediction pipelines in machine learning

---↪️ Techniques for creating prediction pipelines
such as using scikit-learn and TensorFlow

---↪️ Creating end points for prediction pipelines using FastAPI and other tools

☛ **Intro to CI/CD and CT:**:

---↪️ Understanding the importance of continuous integration || delivery || and training in machine learning

---↪️ Techniques for implementing continuous integration || delivery || and training using GitHub Actions and other tools *(in local setup)*

---↪️ Best practices for continuous integration || delivery || and training in machine learning projects

# Module 4: Machine Learning Fundamentals

☛ Introduction to Machine Learning

☛ Types of Machine Learning: Supervised || Unsupervised || Reinforcement Learning

☛ Data Preprocessing || Processing || Transformation

☛ Machine Learning algorithms: Linear Regression || Decision Trees || Random Forest

☛ Model evaluation metrics: Accuracy || Precision || Recall || F1 Score

☛ Model selection and hyperparameter tuning

☛ Model deployment and monitoring with FastAPI/Flask

# Module 5: Deep Learning Fundamentals

☛ Introduction to Deep Learning

☛ Core Neural Network Architectures: CNNs || RNN || GANs

---↪️ Neural Networks || Autoencoders || Generative Adversarial Networks

☛ Activation Functions and Layer Essentials

☛ Training Neural Networks: Backpropagation and Optimization

☛ Deep Learning Frameworks and Tools: TensorFlow || PyTorch || Keras

☛ Deep Network Training Challenges and Solutions

☛ Hyperparameter Optimization and Fine Tuning Deep Learning Models

☛ Deep Learning Hardware and Infrastructure

☛ Deep Learning for Structured and Unstructured Data Types

# Module 6: Fundamentals of Computer Vision

☛ Introduction to Computer Vision

☛ Key Computer Vision Tasks: Filtering || Thresholding || Edge Detection

☛ Feature Extraction

☛ Image processing techniques:

---↪️ Object detection and recognition

---↪️ Deep Learning Applications in Computer Vision

------➡️Batch || Real-time CV Applications

☛ Computer Vision Toolbox: OpenCV || TensorFlow || and More

☛ Challenges in Computer Vision

☛ **Lab: Object Detection with YOLOv5/v8**

# Module 7: Fundamentals of Natural Language Processing

☛ Introduction to NLP

☛ Syntax || semantics || and pragmatics.

☛ Data Preparation and word embeddings

☛ Deep Learning in NLP

☛ NLP Toolbox

☛ Fundamentals of RNN || Transformers. LLM Architectures

☛ Future Directions in NLP

# Module 8: Docker and Containerization for ML Engineers

☛ Introduction to Docker

☛ Docker basics: Images || Containers || Volumes

☛ Container Networking

☛ Redis Deployment and Integration

☛ Kafka Deployment and Integration

☛ Elasticsearch Deployment and Integration

☛ Docker for ML: Building and running ML containers

☛ Docker best practices: Image optimization || Containerization

☛ Explore Over 30 Hands-On Docker Lab Scenarios in the Poridhi Lab Environment

# Module 9: AWS Cloud Fundamentals for ML Workflow

☛ Introduction to AWS

☛ AWS Networking

☛ AWS basics: EC2 || S3 || Lambda

☛ AWS for ML: Deploying ML models to AWS

☛ AWS best practices: Cost optimization || Security

☛ Explore Over 40 Hands-On AWS and ML Scenarios in the Poridhi Lab Environment

# Module 10: ML Workflows with AWS SageMaker

☛ Introduction to end-to-end MLOps platforms

☛ Introduction to Amazon SageMaker: ML development and deployment (Lab)

☛ Explore Over 50 Hands-On AWS Lab Scenarios in the Poridhi Lab Environment

# Module 11: Kubernetes for Scaling ML Workloads

☛ Introduction to Kubernetes

☛ Kubernetes Architecture And Theory

☛ Kubernetes Basics: Pods || Services || Deployments

☛ Deploy k3s Cluster in AWS

☛ Working with Poridhi Kubernetes Cluster

☛ Understanding Kubernetes Networking For Debugging

☛ Kubernetes for ML: Deploying ML models to Kubernetes

☛ Kubernetes best practices: Resource allocation || Scalability

☛ Explore Over 100 Hands-On kubernetes Lab Scenarios in the Poridhi Lab Environment

# Module 12: ML Workflows with KubeFlow

☛ Deploying KubeFlow on Poridhi Kubernetes Clusters/EKS

☛ End-to-end ML Workflow with KubeFlow

# Module 13: Introduction to Ray

☛ Challenges in Traditional Machine Learning Workflows

☛ Benefits of Distributed Computing in Machine Learning

☛ How Ray Addresses These Challenges

☛ Ray Architecture:

---↪️Core Components of Ray:

------➡️ Ray Core

------➡️ Ray Tune

------➡️ Ray Serve

------➡️ Ray Train

☛ Ray's Distributed Scheduler

☛ Actors and Tasks

☛ Raylets || Object Store and Shared Memory

☛ Execution and Resource Management

☛ Overview of ML workflow on Ray

# Module 14: Deploying and Setting up Ray Clusters

☛ Deploying and Managing Clusters

☛ Setting up Ray Clusters on Local Machine

☛ Setting up Ray Clusters on Cloud (AWS)

☛ Autoscaling

# Module 15: Deploying Ray Clusters on KubeRay

☛ Setting up Ray Clusters on Kubernetes:

---↪️ AWS EKS Cluster

---↪️ Poridhi Kubernetes Cluster

# Module 16: Setting up Monitoring and Observability for Ray Clusters

☛ Setting up Ray Dashboard

☛ Setting up Prometheus for Ray Clusters

☛ Setting up Grafana for Ray Clusters

# Module 17: Building a Python App with Ray Core API

☛ Introduction to Ray Core API:

---↪️ Overview of Ray Core API

---↪️ Use Cases for Ray Core API

☛ Implementing Ray Core Concepts:

-↪️ Tasks and Remote Functions:

------➡️ Defining and Using Remote Functions

------➡️ Task Scheduling and Execution

-↪️ Actors:

------➡️ Creating and Managing Actors

------➡️ State Management with Actors -↪️ Object Store:

------➡️ Sharing and Accessing Data

------➡️ Optimizing Object Storage Usage

☛ Building A Ray Application : Deploying a Flask/Django App on Ray Cluster

Module 18: Data Concepts and Data Processing with Ray Datasets

☛ Overview of Distributed Data Processing

☛ Distributed Data Processing with Ray Datasets:

---↪️ Data Ingestion and Loading

---↪️ Data Transformation and Manipulation

---↪️ Data Export and Storage

---↪️ Advanced Data Transformations

☛ Parallel Processing with Ray

☛ Scaling Data Workloads

☛ Optimizing Performance:

---↪️ Data Persistence and Checkpointing

---↪️ Efficient Data Partitioning

---↪️ Minimizing Data Shuffling

---↪️ Leveraging Caching

☛ Integrating Ray Datasets with other Libraries

☛ Designing Efficient Data Pipelines

☛ Fault Tolerance and Recovery

# Module 19: Experiment Tracking and Model Metadata Management Tools

☛ Introduction to experiment tracking and model metadata management

☛ MLflow with Ray Clusters: Experiment tracking || Model versioning || Model serving

☛ Weights & Biases with Ray Clusters: Experiment tracking || Model versioning || Model serving

☛ Setting up Tensorboard for Ray Clusters

☛ Best practices for experiment tracking and model metadata management

# Module 20: Training Large Scale ML Models with Ray Train

☛ Understanding Ray Train Concepts:

---↪️ Overview of Distributed Training

---↪️ Core Components of Ray Train

---↪️ Parallel and Distributed Training Strategies

☛ Data Preparation for Large Scale Training:

---↪️ Loading and Preprocessing Data

---↪️ Efficient Data Partitioning

---↪️ Handling Large Datasets

☛ Model Training with Ray Train:

---↪️ Defining and Configuring Models

---↪️ Using Ray Train API for Training

---↪️ Implementing Custom Training Loops

☛ Distributed Training Techniques:

---↪️ Data Parallelism

---↪️ Model Parallelism

---↪️ Hybrid Parallelism

☛ Integrating with Popular ML Frameworks:

---↪️Using Ray Train with TensorFlow

---↪️ Using Ray Train with PyTorch

---↪️ Integrating with Other ML Libraries

☛ Model Evaluation and Validation:

---↪️Evaluating Model Performance

---↪️ Implementing Validation and Testing

---↪️ Cross-Validation Strategies

☛ Best Practices for Training Large Scale Models with Ray Train:

---↪️Designing Efficient Training Pipelines

---↪️Managing Cluster Resources

---↪️ Ensuring Model Quality and Consistency

# Module 21: Data and Pipeline Versioning Tools

☛ Introduction to data and pipeline versioning

☛ DVC with Ray Clusters: Data versioning || Data management || Data collaboration

☛ Pachyderm: Data versioning || Data pipelines

☛ Best practices for data and pipeline versioning


# Module 22: Hyperparameter Tuning/Optimization with Ray Tune

☛ Understanding Ray Tune Concepts:

---↪️ Overview of Hyperparameter Tuning

---↪️ Core Components of Ray Tune

---↪️ Parallel and Distributed Search Strategies

☛ Defining Hyperparameter Search Space:

---↪️ Specifying Hyperparameters

---↪️ Defining Search Space and Distributions

---↪️Configuring Search Algorithms

☛ Search Algorithms and Schedulers:

---↪️ Random Search

---↪️ Grid Search

---↪️ Bayesian Optimization

---↪️ Hyperband and ASHA

---↪️ Population Based Training (PBT)

☛ Running Hyperparameter Tuning Jobs:

---↪️ Setting Up Training Functions

---↪️ Using Ray Tune API for Tuning

---↪️ Implementing Custom Schedulers

☛ Advanced Hyperparameter Tuning Techniques:

---↪️ Multi-Objective Optimization

---↪️ Transfer Learning for Tuning

---↪️ Early Stopping Strategies

☛ Integrating Ray Tune with ML Frameworks:

---↪️ Ray Tune with TensorFlow

---↪️ Ray Tune with PyTorch

---↪️ Integrating with Other ML Libraries

☛ Monitoring and Analyzing Tuning Jobs:

---↪️ Using Ray Dashboard and Integrated Tools for Monitoring

---↪️ Logging and Visualization Tools

---↪️ Analyzing Hyperparameter Search Results ☛ Performance Optimization:

---↪️ Profiling and Debugging Tuning Jobs

---↪️ Optimizing Resource Utilization

# Module 23: Training Reinforcement Learning Models with Ray RLlib

☛ Understanding Ray RLlib Concepts:

---↪️ Overview of Reinforcement Learning

---↪️ Core Components of RLlib

---↪️ Policies and Algorithms

☛ Defining and Configuring RL Environments:

---↪️ Supported Environments (e.g. OpenAI Gym)

---↪️ Custom Environments

---↪️ Environment Configuration

☛ Implementing RL Algorithms:

---↪️ DQN PPO A3C and More

---↪️ Customizing and Extending Algorithms:

---↪️ Policy Training and Evaluation

☛ Advanced Features and Techniques:

---↪️ Multi-Agent Training

---↪️ Model Parallelism

---↪️ Curriculum Learning

☛ Best Practices for RL Training with RLlib:

---↪️ Designing Efficient RL Pipelines

---↪️ Managing Cluster Resources

---↪️ Ensuring Reproducibility

# Module 24: Deploying and Serving Models with Ray Serve

☛ Introduction to model deployment patterns and serving

☛ Best practices and tools for model deployment and serving

☛ TensorFlow Serving: Serving TensorFlow models

☛ FastAPI in Depth

☛ ONNX Runtime: Serving ONNX models

☛ Ray Serve: Scalable model serving

☛ Understanding Ray Serve Concepts:

---↪️ Core Components of Ray Serve

---↪️ Deployment Graphs and Pipelines

---↪️ Managing Endpoints

☛ Defining and Deploying Models:

---↪️ Preparing Models for Deployment

---↪️ Using Ray Serve API for Deployment

---↪️ Handling Multiple Models and Versions

☛ Routing and Load Balancing:

---↪️Configuring Routing Policies

---↪️ Implementing Load Balancing Strategies

---↪️ Ensuring High Availability

☛ Scalable Model Serving:

---↪️Horizontal and Vertical Scaling

---↪️ Auto-scaling Based on Traffic

---↪️ Managing Resource Allocation

☛ Monitoring and Management:

---↪️ Using Ray Dashboard for Monitoring

---↪️ Logging and Metrics Collection

---↪️ Managing Model Lifecycles

☛ Performance Optimization:

---↪️ Profiling and Debugging Serving Pipelines

---↪️ Reducing Latency and Improving Throughput

---↪️ Optimizing Resource Utilization

☛ Fault Tolerance and Reliability:

---↪️ Ensuring Fault Tolerance in Serving

---↪️ Handling Failures and Recovery

---↪️ Implementing Health Checks and Retries

☛ Common Pitfalls and How to Avoid Them:

---↪️ Debugging Common Issues in Model Serving

---↪️ Handling Large Model Sizes

---↪️ Avoiding Bottlenecks in Serving Pipeline

# Module 25: Model Monitoring in Ray Clusters

☛ Introduction to model monitoring

☛ Best practices for model monitoring:

---↪️ Why Monitoring Matters

---↪️ Observability in ML Model

---↪️ Monitoring Targets in ML

---↪️ Logging for ML Model Monitoring

---↪️ Tracing for ML Systems

---↪️Monitoring Machine Learning Models in Production

---↪️ Model Monitoring and Logging

☛ Prometheus: Metrics collection and alerting

☛ Grafana: Dashboarding and visualization

☛ Evidently AI: Data and model drift detection

# Module 26: Introduction to LLMs

☛ Introduction to LLMs

☛ LLM architectures: Transformer GPT BERT T5 LLama

☛ Transformer Architecture:

---↪️ Key Components: Self-Attention || Multi-head Attention

---↪️ Transformer Encoder and Decoder

☛ Building an LLM from Scratch with Ray:

---↪️ Fundamentals of Natural Language Processing (NLP)

---↪️ Data Collection and Preprocessing

---↪️ Model Architecture Design

---↪️ Training Strategies and Techniques

# Module 27: Building Scalable RAG Applications with Ray and LangChain

☛ LLMOps Libraries & Framework: LangChain

LangServe

☛ Opensource LLMs and HuggingFace Ecosystem

☛ Vector Databases

☛ Fine-tuning LLMs

☛ LLM applications: Text generation || Question answering || Summarization


# Module 28: Introduction to Scalable Agentic Workflows

☛ Introduction to Agentic Workflows with LangChain and Autogen

**Project: Deploying an Agentic Workflow for a Repetitive Task**

# Module 29: Feature Stores and Databases:

☛ Introduction to feature stores

☛ Feast: Data storage || Data processing || Data serving

☛ Tecton: Feature engineering || Feature serving

☛ Best practices for feature engineering and management

The followings may need to go up in the sequence of modules:

---↪️ Setting up Apache Kafka

---↪️ Setting up Elasticsearch

---↪️Setting up MySQL || MongoDB

---↪️ Setting up Redis

# Module 30: Orchestration and Workflow Pipelines

☛ Introduction to orchestration and workflow pipelines

☛ Building CI/CD Pipelines with GitHub Actions

☛ Building Automated Training/Deployment Pipelines

☛ Dagster with Rayr: Pipelines || Tasks || Dependencies

☛ Best practices for orchestrating ML workflows with Ray and Dagster/Airflow